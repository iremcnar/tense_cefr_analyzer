{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb351e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "\n",
    "# Veri setini yükleme\n",
    "df = pd.read_csv('/content/sample_data/clean_questions.csv')\n",
    "\n",
    "# Veri ön işleme\n",
    "def preprocess_data(df):\n",
    "    # Soru ve şıkları birleştirme\n",
    "    df['text'] = df.apply(lambda row: f\"Question: {row['question']} \" +\n",
    "                                     f\"Choices: 1) {row['choice_1']} \" +\n",
    "                                     f\"2) {row['choice_2']} \" +\n",
    "                                     f\"3) {row['choice_3']} \" +\n",
    "                                     f\"4) {row['choice_4']}\", axis=1)\n",
    "\n",
    "    # Cevap etiketlerini 0-3 aralığına düşürme (1-4 yerine)\n",
    "    df['label'] = df['answer'] - 1\n",
    "\n",
    "    return df[['text', 'label']]\n",
    "\n",
    "processed_df = preprocess_data(df)\n",
    "\n",
    "# Eğitim ve test setlerine ayırma\n",
    "train_df, test_df = train_test_split(processed_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c289a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer'ını yükleme\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Veriyi BERT için uygun formata dönüştürme\n",
    "def convert_data_to_examples(df):\n",
    "    examples = []\n",
    "    for index, row in df.iterrows():\n",
    "        examples.append(\n",
    "            InputExample(\n",
    "                guid=None,\n",
    "                text_a=row['text'],\n",
    "                label=row['label']\n",
    "            )\n",
    "        )\n",
    "    return examples\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []\n",
    "\n",
    "    for example in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            example.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (\n",
    "            input_dict['input_ids'],\n",
    "            input_dict['token_type_ids'],\n",
    "            input_dict['attention_mask']\n",
    "        )\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                label=example.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for feature in features:\n",
    "            yield (\n",
    "                {\n",
    "                    'input_ids': feature.input_ids,\n",
    "                    'attention_mask': feature.attention_mask,\n",
    "                    'token_type_ids': feature.token_type_ids\n",
    "                },\n",
    "                feature.label\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        (\n",
    "            {\n",
    "                'input_ids': tf.int32,\n",
    "                'attention_mask': tf.int32,\n",
    "                'token_type_ids': tf.int32\n",
    "            },\n",
    "            tf.int64\n",
    "        ),\n",
    "        (\n",
    "            {\n",
    "                'input_ids': tf.TensorShape([None]),\n",
    "                'attention_mask': tf.TensorShape([None]),\n",
    "                'token_type_ids': tf.TensorShape([None])\n",
    "            },\n",
    "            tf.TensorShape([])\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Veriyi dönüştürme\n",
    "train_examples = convert_data_to_examples(train_df)\n",
    "test_examples = convert_data_to_examples(test_df)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(train_examples, tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "test_data = convert_examples_to_tf_dataset(test_examples, tokenizer)\n",
    "test_data = test_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973fd08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "# Hızlı model ve tokenizer\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Veri hazırlama (daha kısa maksimum uzunluk)\n",
    "def create_dataset(df, tokenizer, max_length=64):\n",
    "    encodings = tokenizer(\n",
    "        df['text'].tolist(),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings),\n",
    "        df['label'].values\n",
    "    )).batch(32)  # Daha büyük batch\n",
    "\n",
    "train_dataset = create_dataset(train_df, tokenizer).shuffle(100)\n",
    "test_dataset = create_dataset(test_df, tokenizer)\n",
    "\n",
    "# Mixed precision (GPU varsa)\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Kısa eğitim\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=5,  # Epoch sayısını azalt\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model ve tokenizer yükleme\n",
    "model_path = r\"C:\\Users\\PC\\Downloads\\english_exam_model\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Veri setini yükleme\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.dropna(subset=['question', 'choice_1', 'choice_2', 'choice_3', 'choice_4', 'answer'])\n",
    "    return df\n",
    "\n",
    "# Rastgele soru seçme\n",
    "def get_random_question(df):\n",
    "    random_row = df.sample(1).iloc[0]\n",
    "    question = random_row['question']\n",
    "    choices = {\n",
    "        1: random_row['choice_1'],\n",
    "        2: random_row['choice_2'],\n",
    "        3: random_row['choice_3'],\n",
    "        4: random_row['choice_4']\n",
    "    }\n",
    "    correct_answer = random_row['answer']\n",
    "    return question, choices, correct_answer\n",
    "\n",
    "# Model tahmini yapma\n",
    "def predict_answer(model, tokenizer, question, choices):\n",
    "    text = f\"Question: {question} Choices: 1) {choices[1]} 2) {choices[2]} 3) {choices[3]} 4) {choices[4]}\"\n",
    "    inputs = tokenizer(text, return_tensors='tf', truncation=True, max_length=128)\n",
    "    outputs = model(inputs)\n",
    "    predicted_answer = tf.argmax(outputs.logits, axis=1).numpy()[0] + 1\n",
    "    return predicted_answer\n",
    "\n",
    "# Oyun döngüsü\n",
    "def quiz_game(df, model, tokenizer):\n",
    "    print(\"\\n--- İngilizce Test Uygulaması ---\")\n",
    "    print(\"Çıkmak için 'q' girin\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question, choices, correct_answer = get_random_question(df)\n",
    "        \n",
    "        print(f\"\\nSoru: {question}\")\n",
    "        for num, choice in choices.items():\n",
    "            print(f\"{num}) {choice}\")\n",
    "        \n",
    "        user_input = input(\"\\nCevabınız (1-4): \")\n",
    "        \n",
    "        if user_input.lower() == 'q':\n",
    "            print(\"Çıkış yapılıyor...\")\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            user_answer = int(user_input)\n",
    "            if user_answer not in [1, 2, 3, 4]:\n",
    "                print(\"Lütfen 1-4 arasında bir numara girin!\")\n",
    "                continue\n",
    "                \n",
    "            # Model tahmini\n",
    "            predicted = predict_answer(model, tokenizer, question, choices)\n",
    "            \n",
    "            if user_answer == correct_answer:\n",
    "                print(\"✅ Doğru cevap!\")\n",
    "            else:\n",
    "                print(f\"❌ Yanlış cevap! Doğru cevap: {correct_answer}) {choices[correct_answer]}\")\n",
    "            \n",
    "            print(f\"Model tahmini: {predicted}) {choices.get(predicted, 'N/A')}\")\n",
    "            \n",
    "        except ValueError:\n",
    "            print(\"Geçersiz giriş! Lütfen numara girin.\")\n",
    "\n",
    "# Ana program\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = load_data(r\"C:\\Users\\PC\\OneDrive\\Belgeler\\grammer_classification\\clean_questions.csv\")\n",
    "        quiz_game(df, model, tokenizer)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Hata: clean_questions.csv dosyası bulunamadı!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Beklenmeyen bir hata oluştu: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
